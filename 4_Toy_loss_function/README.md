# Simple optimization problem

## Introduction

This repo is modified form the [project](https://github.com/ivallesp/awesome-optimizers).

The play ground demostrate varing problems and gradient-basd optimizers.

In this framework, optimizers listed below are available:
- **Gradient Descent with Nesterov Momentum**
- **RMSProp**
- **Adam**
- **Adabelief**
- **Frankenstein**

Optimization problems listed below are available as well:
- **Beale**
- **Rose**
- **Booth**
- **saddle**
- **ssine**

## Getting started

```
python main.py

#in the code

#name='%s' %(name of problems mentioned above)
#ex. name='Rose'
#learning_rate='%f' %(the step size of all optimizers)
#ex. learning_rate=1e-2
```

## Result
###Optimizers with learning rate 1e-2 on Beale problem
<div align="center">
      <a href="https://youtu.be/aE1_6LLsP18">
         <img src="https://youtu.be/aE1_6LLsP18/0.jpg" style="width:100%;">
      </a>
</div>
###Optimizers with learning rate 1e-4 on Beale problem
<div align="center">
      <a href="https://youtu.be/Jju3I3-0ies">
         <img src="https://youtu.be/Jju3I3-0ies/0.jpg" style="width:100%;">
      </a>
</div>
